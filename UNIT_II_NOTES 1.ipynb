{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13d878a-8d6b-402e-b114-f24ccce68f85",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">\n",
    "    \n",
    "# <font color=maroon>Introduction\n",
    "\n",
    "In the culture of statistical inference, particularly within the Frequentist framework, probability is understood in terms of **long-run frequency** — the idea that the probability of an event reflects how often it would occur in a large number of repeated trials. This foundational view supports much of modern data analysis, where conclusions about a **target population** are drawn based on a smaller, representative sample.\n",
    "\n",
    "The **target population** refers to the complete group we aim to understand, but due to practical constraints, data is typically collected from a **sampled population**, ideally chosen at random. A **random sample** ensures that every member of the population has a known and equal chance of selection, which allows the use of probability theory to justify the reliability of inferences. From this sample, we compute **statistics** — such as the sample mean, variance, or proportion — which are numerical summaries based solely on observed data. These statistics are then used to estimate unknown characteristics (parameters) of the population. To evaluate and support these estimations, we rely on **theoretical probability distributions**, which describe how such statistics would behave across many repeated samples. Altogether, these concepts enable us to make reasoned, probabilistic statements about populations using only sample data.\n",
    "\n",
    "\n",
    "    \n",
    "## Sampling Distribution \n",
    "\n",
    "Frequntist probability of an event $\\mathcal{A}$ (from a random experiment, $\\mathcal{E}$) is based on relative frequency (number of times $\\mathcal{A}$ is repeated) in many trials of $\\mathcal{E}$. Here, probability is defined as a repeatable objective way out of \"sufficiently\" long-run of the experiment. This may be understood as\n",
    "\n",
    "$$p(\\mathcal{A})= \\lim_{n \\to \\infty} \\frac{m}{n}$$\n",
    "\n",
    "where $n$ is the number of trials of $\\mathcal{E}$ and $m$ is  the frequency of $\\mathcal{A}$ in $n$ trials\n",
    "\n",
    "This basic notion is considered while building the inferential principles of frequntist statistics. Most of the studies (involved with random experiments) are conducted with samples drawn from a study population which is a part (or subset) of a target population about which the study is aimed to generalize the findings from sample. In this process, data is expected to be a repeatable process in that samples are drawn from many trials. There could be many sampling methods through which data are collected for a reasonable way to get representative samples so as to generealize the quantity of interest (parameter) about the population. Further, it is assemed that parameters are fixed constant but random and **inference** about it can be done via the samples\n",
    "\n",
    "While doing so, it is assuemd that (theoretically) once the quantity leading to the parameter is calculated from each sample for \"sufficiently large\" number of times. That gives a distribution of values for the parameter calculated from the sample which is called **statistic** and the distribution is said to be **Sampling distribution**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
